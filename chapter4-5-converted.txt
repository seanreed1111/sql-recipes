
4-1. Deriving New Columns
Problem
You don’t want to store redundant data in your database tables, but you want to be able to create totals, derived values, or alternate formats for columns within a row.


Solution
In your SELECT statements, apply Oracle built-in functions or create expressions on one or more columns in the table, creating a virtual column in the query results. For example, suppose you want to summarize total compensation for an employee, combining salary and commissions.
In the sample tables included for the OE user from a default installation of Oracle, the ORDER_ITEMS
table contains UNIT_PRICE and QUANTITY columns as follows:
select * from order_items;


665 rows selected
To provide a line-item total in the query results, add an expression that multiplies unit price by quantity, as follows:
select order_id, line_item_id, product_id,
unit_price, quantity, unit_price*quantity line_total_price from order_items;


665 rows selected


How It Works
In a SELECT statement (or any DML statement, such as INSERT, UPDATE, or DELETE), you can reference any Oracle built-in function (or one of your own functions) on any or all columns in the statement. You can include aggregate functions such as SUM() and AVG() as well, as long as you include the non-aggregated columns in a GROUP BY clause.
You can also nest your functions within a SELECT statement. The column LINE_TOTAL_PRICE contains
the desired result, but is not formatted appropriately for displaying to a customer service representative or a customer. To improve the readability of the output, add the TO_CHAR function to the calculation already in the query as follows:

select order_id, line_item_id, product_id, unit_price, quantity,
to_char(unit_price*quantity,'$9,999,999.99') line_total_price from order_items;
ORDER_ID LINE_ITEM_ID PRODUCT_ID UNIT_PRICE QUANTITY LINE_TOTAL_PRICE

665 rows selected
The TO_CHAR function can also be used to convert date or timestamp columns to a more readable or non-default format; you can even use TO_CHAR to convert data types in the national character set (NCHAR, NVARCHAR2, NCLOB) and CLOBs to the database character set, returning a VARCHAR2.
If your users need to run this query often, you might consider creating a database view, which will make it even easier for your analysts and other end-users to access the data. The new view will look like another table with one more column than the original table. Use the CREATE VIEW statement to create a view on the ORDER_ITEMS table, including the derived column:
create view order_items_subtotal_vw as
select order_id, line_item_id, product_id, unit_price, quantity,
to_char(unit_price*quantity,'$9,999,999.99') line_total_price from order_items
;
To access the table’s columns and the derived column from the view, use this syntax:
select order_id, line_item_id, product_id,


unit_price, quantity, line_total_price from order_items_subtotal_vw;

The next time you need to access the derived LINE_TOTAL_PRICE column, you don’t need to remember the calculations or the function you used to convert the total price to a currency format!
Using a database view in this scenario is useful, but adds another object to the data dictionary and thus creates another maintenance point if the base table changes, as when you modify or remove the UNIT_PRICE or QUANTITY columns. If you are using Oracle Database 11g or later, you can use a feature called virtual columns, which essentially creates the metadata for a derived column within the table definition itself. You can create a virtual column when you create a table, or add it later as you would any other column. The virtual column definition takes up no space in the table itself; it is calculated on the fly when you access the table containing the virtual column. To add the virtual column LINE_TOTAL_PRICE to ORDER_ITEMS, use this syntax:
alter table order_items
add (line_total_price as (to_char(unit_price*quantity,'$9,999,999.99')));
The virtual column appears to be a regular column like any of the real columns in the table:
describe order_items;
Name	Null	Type
ORDER_ID	NOT NULL NUMBER(12)
LINE_ITEM_ID	NOT NULL NUMBER(3)
PRODUCT_ID	NOT NULL NUMBER(6)
UNIT_PRICE	NUMBER(8,2)
QUANTITY	NUMBER(8)
LINE_TOTAL_PRICE	VARCHAR2(14)
6 rows selected

The data type of the virtual column is implied by the result of the calculation in the virtual column definition; the result of the expression in the LINE_TOTAL_PRICE virtual column is a character string with a maximum width of 14, thus the data type of the virtual column is VARCHAR2(14).

Tip To find out if a column is a virtual column, you can query the VIRTUAL_COLUMN column of the *_TAB_COLS
data dictionary views.


There are a number of advantages to using virtual columns, such not having to create a view on the table, as we did in the previous example. In addition, you can create indexes on virtual columns and collect statistics on them. Here is an example of creating an index on the virtual column from the previous example:
create index ie1_order_items on order_items(line_total_price);




Note For versions of Oracle that do not support virtual columns (before Oracle Database 11g), you can somewhat simulate the indexing feature of a virtual column by creating a function-based index on the expression used in the virtual column.


There are a few restrictions on virtual columns, some of them obvious, some not so much. If your virtual column contains a built-in or user function, the referenced function must be declared as DETERMINISTIC; in other words, the function must return the same results for the same set of input values (for example, other columns in the row). Therefore, you can’t use functions such as SYSDATE or SYSTIMESTAMP in a virtual column, as they will almost always return different results every time the row is retrieved (unless the results are retrieved within the same second for SYSDATE or millionth of a second for SYSTIMESTAMP!). If you declare a user function to be DETERMINISTIC when it’s not and reference it in a virtual column, you are asking for trouble; your query results that include the virtual column will most likely be incorrect or will produce different results depending on when you run it, who is running it, and so forth.
Other restrictions on virtual columns are not so obvious. A virtual column can’t reference another virtual column in the table (although you can certainly include one virtual column’s definition within the definition of the second virtual column). In addition, the virtual column can only reference columns within the same table. Overall, virtual columns can enhance your database usability while somewhat reducing the amount of metadata you might otherwise have to maintain in a database view.

4-2. Returning Nonexistent Rows
Problem
You have gaps in the sequence numbers used for a table’s primary key, and you want to identify the intermediate sequence numbers.

Solution
Although primary keys are generally (and preferably) invisible to the end user, there are times you may want to reuse sequence numbers where the number of digits in the sequence number must be kept as low as possible to satisfy the requirements of downstream systems or processes. For example, the company baseball team uses the number in EMPLOYEE_ID column as the number on the back of the uniform jersey. The manager wants to reuse old jersey numbers if possible, since the number on the back of the jersey is limited to three digits.
To find the employee numbers that were skipped or are currently not in use, you can use a query with two embedded subqueries as follows:
with all_used_emp_ids as
(select level poss_emp_id from (select max(employee_id) max_emp_num
from employees) connect by level <= max_emp_num)
select poss_emp_id from all_used_emp_ids


where poss_emp_id not in (select employee_id from employees) order by poss_emp_id
;
POSS_EMP_ID
1
2
3
4
5
. . . 99
179
183
101 rows selected

The query retrieves any unused employee numbers up to the highest existing employee number currently in the EMPLOYEES table.

How It Works
The solution uses subquery factoring and two explicit subqueries to find the list of skipped employee numbers. The query in the WITH clause creates a series of numbers (with no gaps) from one to the value of the highest employee number in the EMPLOYEES table. The highlighted part of the subquery that follows retrieves the highest number:
(select level poss_emp_id from (select max(employee_id) max_emp_num
from employees)
connect by level <= max_emp_num)
The rest of the query in the WITH clause generates the gapless sequence of numbers. It leverages a subtle feature of Oracle’s hierarchical query features (the CONNECT BY clause and the LEVEL pseudo- column) to generate each number in the sequence. See Chapter 13 for other useful recipes that require traversing hierarchical data.
Here is the main part of the SELECT query:
select poss_emp_id from all_used_emp_ids
where poss_emp_id not in (select employee_id from employees)
It retrieves all rows from the gapless sequence with possible employee numbers that do not already exist in the EMPLOYEES table. The final result set is ordered by POSS_EMP_ID to make it easy to assign the lowest available number to the next employee. If you want to always exclude one- or two-digit numbers, you can add another predicate to the WHERE clause as follows:
with all_used_emp_ids as
(select level poss_emp_id from (select max(employee_id) max_emp_num
from employees) connect by level <= max_emp_num)


select poss_emp_id from all_used_emp_ids
where poss_emp_id not in (select employee_id from employees) and poss_emp_id > 99
order by poss_emp_id
;
POSS_EMP_ID
179
183
2 rows selected
In the revised query’s results, the only three-digit numbers that can be reused below the highest employee number currently in use are 179 and 183.

4-3. Changing Rows into Columns
Problem
Your transaction data is in a highly normalized database structure, and you want to easily create crosstab-style reports for the business analysts, returning totals or other aggregate results as multiple columns within a single row.

Solution
Use the PIVOT keyword in your SELECT statement to spread values (in one or many columns) from multiple rows aggregated into multiple columns in the query output. For example, in the Oracle order entry sample schema, OE, you want to pick five key products from all orders and find out which customers are buying these products, and how many they bought.
The ORDERS table and ORDER_ITEMS table are defined as follows:
describe orders;
Name	Null	Type
ORDER_ID	NOT NULL NUMBER(12)
ORDER_DATE	NOT NULL TIMESTAMP(6) WITH LOCAL TIME ZONE ORDER_MODE		VARCHAR2(8)
CUSTOMER_ID	NOT NULL NUMBER(6)
ORDER_STATUS	NUMBER(2)
ORDER_TOTAL	NUMBER(8,2)
SALES_REP_ID	NUMBER(6)
PROMOTION_ID	NUMBER(6)
8 rows selected


describe order_items;
Name	Null	Type
ORDER_ID	NOT NULL NUMBER(12)
LINE_ITEM_ID	NOT NULL NUMBER(3)
PRODUCT_ID	NOT NULL NUMBER(6)
UNIT_PRICE	NUMBER(8,2)
QUANTITY	NUMBER(8)
LINE_TOTAL_PRICE	VARCHAR2(14)
6 rows selected
Here is the pivot query to retrieve quantity totals for the top five products by customer:
with order_item_query as
(select customer_id, product_id, quantity
from orders join order_items using(order_id)) select * from order_item_query
pivot (
sum(quantity) as sum_qty
for (product_id) in (3170 as P3170,
3176 as P3176,
3182 as P3182,
3163 as P3163,
3165 as P3165)
)
order by customer_id
;
CUSTOMER_ID P3170_SUM_QTY P3176_SUM_QTY P3182_SUM_QTY P3163_SUM_QTY P3165_SUM_QTY

47 rows selected


From the results of this query, you can easily see that customer number 104 is the top buyer of all of the products specified in the PIVOT clause.

How It Works
The PIVOT operator, as the name implies, pivots rows into columns. In the solution, you will notice a few familiar constructs, and a few not so familiar. First, the solution uses the WITH clause (subquery factoring) to more cleanly separate the base query from the rest of the query; here is the WITH clause used in the solution:
with order_item_query as
(select customer_id, product_id, quantity
from orders join order_items using(order_id))
Whether you use the WITH clause or an inline view is a matter of style. In either case, be sure to only include columns that you will be grouping by, aggregate columns, or columns used in the PIVOT clause. Otherwise, if you have any extra columns in the query, your results will have a lot more rows than you expected! This is much like adding extra columns to a GROUP BY clause in a regular SELECT statement with aggregates, typically increasing the number of rows in the result.
The PIVOT clause itself generates the additional columns in the query results. Here is the first part of the PIVOT clause in the solution:
sum(quantity) as sum_qty
You specify one or more aggregates that you want to appear in the new columns and assign a label suffix that Oracle will use for each new column. (We’ll show you how to pivot on more than one column or aggregate more than one column later in this section.) In this case, all of the derived columns will end in SUM_QTY.
The column or columns in the FOR clause filter the rows that are used to aggregate the results. Each value of the column in the FOR clause, or each specified combination of two or more columns, will produce an additional derived column in the results. The FOR clause in the solution is as follows:
for (product_id) in (3170 as P3170,
3176 as P3176,
3182 as P3182,
3163 as P3163,
3165 as P3165)
The query will only use rows whose PRODUCT_ID is in the list, and thus acts like a WHERE clause to filter the results before aggregation. The text string specified after each pivoted value is used as the prefix for the new column name, as you can see in the solution query’s results.
Finally, the ORDER BY clause does the same thing as it would in any other query—order the final
result set by the columns specified.


4-4. Pivoting on Multiple Columns
Problem
You have implemented the solution shown in the previous recipe. Now you wish to pivot not only on the total quantity of products, but also on the total dollar value purchased.

Solution
The following query extends the previous recipe’s solution to add a total dollar amount column for each product number:
with order_item_query as
(select customer_id, product_id, quantity, unit_price
from orders join order_items using(order_id)) select * from order_item_query
pivot (
sum(quantity) as sum_qty,
sum(quantity*unit_price) as sum_prc
for (product_id) in (3170 as P3170,
3176 as P3176,
3182 as P3182,
3163 as P3163,
3165 as P3165)
)
order by customer_id
;
CUSTOMER_ID P3170_SUM_QTY P3170_SUM_PRC P3176_SUM_QTY P3176_SUM_PRC . . .


This report further reinforces the findings from the original solution: customer number 104 is a big buyer in terms of total dollars paid as well.


How It Works
Once you’ve mastered the basics of the PIVOT clause, taking it a step further is easy. Just add as many additional aggregates to your PIVOT clause as you need. Remember, though, that adding a second aggregate doubles the number of derived columns, adding a third triples it, and so forth.
Let’s extend the solution even more, adding another pivot column. In the ORDERS table, the column ORDER_MODE contains the string direct if the sale was over the phone and ONLINE if the order was placed on the company’s Internet site. To pivot on ORDER_MODE, just add the column to the FOR clause and specify pairs of values instead of single values in the FOR list as follows:
with order_item_query as
(select customer_id, product_id, quantity, unit_price, order_mode from orders join order_items using(order_id))
select * from order_item_query pivot (
sum(quantity) as sum_qty, sum(quantity*unit_price) as sum_prc
for (product_id, order_mode) in ((3170,'direct') as P3170_DIR,
(3170,'online') as P3170_ONL, (3176,'direct') as P3176_DIR, (3176,'online') as P3176_ONL, (3182,'direct') as P3182_DIR, (3182,'online') as P3182_ONL, (3163,'direct') as P3163_DIR, (3163,'online') as P3163_ONL, (3165,'direct') as P3165_DIR, (3165,'online') as P3165_ONL)
)
order by customer_id
;
CUSTOMER_ID P3170_DIR_SUM_QTY P3170_DIR_SUM_PRC P3170_ONL_SUM_QTY P3170_ONL_SUM_PRC


The output has been truncated horizontally for readability. There is one column for each combination of product number, order mode, and the two aggregates. Looking at the breakdown by


order mode, it appears that the company could save money by shifting some or all of customer 104’s orders to Internet orders.
Finally, you might not know your pivot criteria ahead of time, as new products may appear on a daily basis, making maintenance of your queries an issue. One possible solution is to dynamically build your queries using Java or PL/SQL, though this solution may have potential security and performance issues. However, if the consumer of your query output can accept XML output, you can use the PIVOT XML clause instead of just PIVOT, and include filtering criteria in the IN clause as in this example, revising the original solution:
with order_item_query as
(select customer_id, product_id, quantity
from orders join order_items using(order_id)) select * from order_item_query
pivot xml (
sum(quantity) as sum_qty
for (product_id) in (any)
)
order by customer_id
;
CUSTOMER_ID	PRODUCT_ID_XML
	<PivotSet><item><column name = "PRODUCT_ID">2264</column><column name = "SUM_QTY">29</column></item> . . .
	<PivotSet><item><column name = "PRODUCT_ID">2976</column><column name = "SUM_QTY">5</column></item> . . .
	<PivotSet><item><column name = "PRODUCT_ID">1910</column><column name = "SUM_QTY">6</column></item> . . .
. . .
170		<PivotSet><item><column name = "PRODUCT_ID">3106</column><column name = "SUM_QTY">170</column></item>
47 rows selected
The ANY keyword is shorthand for SELECT DISTINCT PRODUCT_ID FROM ORDER_ITEM_QUERY. Instead of ANY, you can put in just about any SQL statement that returns values in the domain you’re pivoting on, as in this example where you only want to display results for products that are currently orderable:
with order_item_query as
(select customer_id, product_id, quantity
from orders join order_items using(order_id)) select * from order_item_query
pivot xml (
sum(quantity) as sum_qty
for (product_id) in (select distinct product_id
from product_information
where product_status = 'orderable')
)
order by customer_id
;


Note that you need to include the DISTINCT keyword in your IN clause to ensure that you have only unique values on the pivot columns. Otherwise, the pivot operation will return an error.

4-5. Changing Columns into Rows
Problem
You have a table with multiple columns containing data from the same domain, and you want to convert these columns into rows of a table with a more normalized design.

Solution
Use the UNPIVOT operator after the FROM clause to convert columns into rows.
In the following example, a web form allows a registered account user to sign up for a free vacation holiday with up to three friends. As you might expect, the direct marketing department wants to use the e-mail addresses for other promotions as well. The format of the table (designed by analysts with minimal database design skills!) is as follows:

create table email_signup (user_account		varchar2(100), signup_date	date,
user_email varchar2(100), friend1_email varchar2(100), friend2_email varchar2(100), friend3_email varchar2(100))
;
The registration data in the first two rows of the table looks like this:
USER_ACCOUNT SIGNUP_DATE USER_EMAIL	FRIEND1_EMAIL FRIEND2_EMAIL FRIEND3_EMAIL



2 rows selected
Unfortunately, the direct marketing application needs the e-mail address list in a more normalized format as follows:
REQUEST_ACCOUNT | REQUEST_DATE | EMAIL_ADDRESS
To generate the e-mail list in the format that the direct marketing group needs, you can use the
UNPIVOT command like so:


select user_account, signup_date, src_col_name, friend_email from email_signup
unpivot (
(friend_email) for src_col_name
in (user_email, friend1_email, friend2_email, friend3_email)
)
;
USER_ACCOUNT SIGNUP_DATE SRC_COL_NAME FRIEND_EMAIL

6 rows selected.

How It Works
In contrast to the PIVOT operator, the UNPIVOT operator changes columns into multiple rows, although you can’t reverse engineer any aggregated column totals without the original data—and if you had the original data, you would not need to UNPIVOT! There are many uses for UNPIVOT, as in converting data in a denormalized table or spreadsheet to individual rows for each column or set of columns.
The direct marketing department does not need the column SRC_COL_NAME, but it can come in handy when you want to find out which source column the e-mail address came from. You can also use UNPIVOT to create more than one destination column; for example, the web form may include a field for each friend’s name in addition to their e-mail address:

select user_account, signup_date, src_col_names, friend_email, friend_name from email_signup
unpivot (
(friend_email,friend_name) for src_col_names in ((user_email,user_name),
(friend1_email,friend1_name), (friend2_email,friend2_name), (friend3_email,friend3_name))
)
;


Before Oracle Database 11g, your options were somewhat limited and painful, as the PIVOT and UNPIVOT clauses did not exist! To perform a PIVOT query, you had to use a series of DECODE statements or make a very complicated foray into the MODEL analytical clause.



Performing an UNPIVOT operation before Oracle Database 11g was nearly as painful. Here is a pre-Oracle Database 11g UNPIVOT operation on the e-mail address list processing table from the previous section:
select user_account, signup_date,
'USER_EMAIL' as src_col_name, user_email as friend_email from email_signup
where user_email is not null union
select user_account, signup_date, 'FRIEND1_EMAIL', friend1_email from email_signup
where friend1_email is not null union
select user_account, signup_date, 'FRIEND2_EMAIL', friend2_email from email_signup
where friend2_email is not null union
select user_account, signup_date, 'FRIEND3_EMAIL', friend3_email from email_signup
where friend3_email is not null
;
The maintenance cost of this query is much higher and the execution time much longer due to the sorting and merging required with the series of UNION statements and multiple passes over the same table.


4-6. Concatenating Data for Readability
Problem
For reporting and readability purposes, you want to combine multiple columns into a single output column, eliminating extra blank space and adding punctuation where necessary.

Solution
Use Oracle string concatenation functions or operators to save space in your report and make the output more readable. For example, in the EMPLOYEES table, you can use the || (two vertical bars) operator or the CONCAT function to combine the employee’s first and last name:

select employee_id, last_name || ', ' || first_name full_name, email from employees
;






107 rows selected
The query concatenates the last name, a comma, and the first name into a single string, aliased as FULL_NAME in the results. If your platform’s character set does not support using || as a concatenation operator (as some IBM mainframe character sets do), or you might soon migrate your SQL to such a platform, you can make your code more platform-independent by using the CONCAT functions instead:

select employee_id, concat(concat(last_name,', '),first_name) full_name, email from employees
;
Because the CONCAT function only supports two operands as of Oracle Database 11g, concatenating more than two strings can make the code unreadable very fast!

How It Works
You can apply a number of different Oracle built-in functions to make your output more readable; some Oracle shops I’ve worked in relied solely on SQL*Plus for their reporting. Applying the appropriate functions and using the right SQL*Plus commands makes the output extremely readable, even with queries returning Oracle objects, currency columns, and long character strings.
If your text-based columns have leading or trailing blanks (which of course should have been cleaned up on import or by the GUI form), or the column is a fixed-length CHAR column, you can get rid of leading and trailing blanks using the TRIM function, as in this example:

select employee_id, trim(last_name) || ', ' || trim(first_name) full_name, email from employees
;
If you only want to trim leading or trailing blanks, you can use LTRIM or RTRIM respectively. If some of your employees don’t have first names (or last names), your output using any of the previous solutions would look a little odd:

select employee_id, last_name || ', ' || first_name full_name, email from celebrity_employees
;




To remedy this situation, you can add the NVL2 function to your SELECT statement:
select employee_id, trim(last_name) ||
nvl2(trim(first_name),', ','') || trim(first_name) full_name,
email
from employees
;
The NVL2 function evaluates the first argument TRIM(FIRST_NAME). If it is not NULL, it returns ', ', otherwise it returns a NULL (empty string), so that our celebrity employees won’t have a phantom comma at the end of their name:



Finally, you can also take advantage of the INITCAP function to fix up character strings that might have been entered in all caps or have mixed case. If your EMPLOYEES table had some rows with last name and first name missing, you could derive most of the employee names from the e-mail address by using a combination of SUBSTR, UPPER, and INITCAP as in this example:
select employee_id, email,
upper(substr(email,1,1)) || ' ' || initcap(substr(email,2)) name from employees
;



This solution is not ideal if the e-mail address does not contain the complete employee name or if the employee’s last name has two parts, such as McDonald, DeVry, or DeHaan.


4-7. Translating Strings to Numeric Equivalents
Problem
In an effort to centralize your domain code management and further normalize the structure of your database tables, you want to clean up and convert some text-format business attributes to numeric equivalents. This will enhance reporting capabilities and reduce data entry errors in the future.

Solution
Use the CASE function to translate business keys or other intelligent numeric keys to numeric codes that are centrally stored in a domain code table. For example, the ORDERS table of the OE schema contains a column ORDER_MODE that currently has four possible values, identified in Table 4-1.

Table 4-1. Mapping the Text in the ORDER_MODE Column to Numeric Values



The second column of Table 4-1 contains the numeric value we want to map to for each of the possible values in the ORDER_MODE column. Here is the SQL you use to add the new column to the table:
alter table orders add (order_mode_num number);
Oracle versions 9i and later include the CASE statement, which is essentially a way to more easily execute procedural code within the confines of the typically non-procedural SQL command language. The CASE statement has two forms: one for simpler scenarios with a single expression that is compared to a list of constants or expressions, and a second that supports evaluation of any combination of columns and expressions. In both forms, CASE returns a single result that is assigned to a column in the SELECT query or DML statement.
The recipe solution using the simpler form of the CASE statement is as follows:
update orders
set order_mode_num = case order_mode
when 'direct' then 1 when 'online' then 2 when 'walmart' then 3


when 'amazon' then 4 else 0
end
;
Once you run the UPDATE statement, you can drop the ORDER_MODE column after verifying that no other existing SQL references it.

How It Works
The CASE statement performs the same function as the older (but still useful in some scenarios) DECODE function, and is a bit more readable as well. For more complex comparisons, such as those evaluating more than one expression or column, you can use the second form of the CASE statement. In the second form, the CASE clause does not contain a column or expression; instead, each WHEN clause contains the desired comparison operation. Here is an example where we want to assign a special code of 5 when the order is an employee order (the CUSTOMER_ID is less than 102):
update orders
set order_mode_num = case
when order_mode = 'direct' and customer_id < 102 then 5
when order_mode = 'direct' then 1 when order_mode = 'online' then 2 when order_mode = 'walmart' then 3 when order_mode = 'amazon' then 4 else 0
end
;

Note that in this scenario you need to check for the employee order first in the list of WHEN clauses, otherwise the ORDER_MODE column will be set to 1 and no customer orders will be flagged, since both conditions check for ORDER_MODE = 'direct'. For both DECODE and CASE, the evaluation and assignment stops as soon as Oracle finds the first expression that evaluates to TRUE.
In the solution, the string 'direct' is translated to 1, 'online' is translated to 2, and so forth. If the ORDER_MODE column does not contain any of the strings in the list, the ORDER_MODE_NUM column is assigned 0.
Finally, reversing the mapping in a SELECT statement for reporting purposes is very straightforward: we can use CASE or DECODE with the text and numeric values reversed. Here is an example:

select order_id, customer_id, order_mode_num, case order_mode_num
when 1 then 'Direct, non-employee' when 2 then 'Online'
when 3 then 'WalMart' when 4 then 'Amazon'
when 5 then 'Direct, employee' else 'unknown'
end order_mode_text


from orders
where order_id in (2458,2397,2355,2356)
;


4 rows selected
The older DECODE statement is the most basic of the Oracle functions that converts one set of values to another; you can convert numeric codes to human-readable text values or vice versa, as we do in the previous solutions. DECODE has been available in Oracle since the earliest releases. DECODE has a variable number of arguments, but the arguments can be divided into three groups:
The column or expression to be translated
One or more pairs of values; the first value is the existing value and the second is the translated value
A single default value if the column or expression to be translated does not match the first value of any of the specified pairs
Here is the UPDATE statement using DECODE:
update orders
set order_mode_num = decode(order_mode,
'direct',1,
'online',2,
'walmart',3,
'amazon',4,
0)
;
105 rows updated
DECODE translates the ORDER_MODE column just as CASE does. If the values in the column do not match any of the values in the first of each pair of constants, DECODE returns 0.

4-8. Generating Random Data
Problem
You need to generate random numbers to simulate real-world events that do not follow a discernible pattern.


Solution
Use the Oracle built-in PL/SQL package DBMS_RANDOM. The RANDOM function returns an integer in the range [-231,231) (-231 can be returned, but 231 will not), and the VALUE function returns a decimal number in the range [0,1) with 38 digits of precision.
For example, the merchandising department wants to lower pricing below list price on catalog items on a daily basis to potentially stimulate sales from customers who perceive a bargain when the price is at least 10 percent below list price. However, the merchandising analysts want to vary the discount from 10 percent to 20 percent on a random basis. To do this, first create a DAILY_PRICE column in the PRODUCT_INFORMATION table as follows:
alter table product_information add (daily_price number);
Next, use the DBMS_RANDOM.VALUE function to adjust the DAILY_PRICE to a value between 10 percent and 20 percent below the list price:

update product_information set daily_price =
round(list_price*(0.9-(dbms_random.value*0.1)))
;
288 rows updated.
Here is the query to retrieve the calculated daily price:

select product_id, list_price, daily_price from product_information
;



Running the UPDATE statement a second time will adjust the prices randomly again, with a different discount for each product, but still ranging from 10 to 20 percent off:

update product_information set daily_price =
round(list_price*(0.9-(dbms_random.value*0.1)))
;
select product_id, list_price, daily_price from product_information
;




The random number value returned is multiplied by 0.1 (10 percent), subtracted from 0.9 (90 percent of the list price is 10 percent off), resulting in a discount of between 10 and 20 percent. The final result is rounded to the nearest dollar.

How It Works
Most, if not all, random number generators start generating the results with a seed value, in other words, a value that the random number generator uses to calculate a starting place in the random number sequence. The DBMS_RANDOM package is no exception. If you do not specify a seed value, Oracle uses the current date, user ID, and process ID to calculate the seed. To reproduce a sequence of random numbers from the same starting point, you can specify your own seed, as in this example:
begin
dbms_random.seed(40027); end;

The DBMS_RANDOM.SEED procedure will also accept a string value to initialize the random number sequence:
begin
dbms_random.seed('The Rain in Spain'); end;

The DBMS_RANDOM.VALUE function has even a bit more flexibility: you can specify that the random number fall within the specified range. For example, if you only want random numbers between 50 and 100, you can do this:
select dbms_random.value(50,100) from dual; DBMS_RANDOM.VALUE(50,100)

95.2813813550075919354971754613182604395
1 rows selected

As a result, you can modify the recipe solution a bit as follows to take advantage of the range specification:
update product_information


set daily_price =
round(list_price*(0.9-(dbms_random.value(0.0,0.1))))
;
Finally, the DBMS_RANDOM package also includes the STRING function so you can retrieve random string values. You specify two parameters: the type of string returned, and the length. Here is an example:
select dbms_random.string('U',20) from dual; DBMS_RANDOM.STRING('U',20)

FTFGYZPCYWNCOKYCKDJI
1 rows selected
The first parameter can be one of these string constants:
‘u’ or ‘U’: uppercase alpha
‘l’ or ‘L’: lowercase alpha
‘a’ or ‘A’: mixed case alpha
‘x’ or ‘X’: upper case alphanumeric
‘p’ or ‘P’: any printable characters
Thus, to generate a very cryptic and random set of 25 characters, you can do this:
select dbms_random.string('p',25) from dual; DBMS_RANDOM.STRING('P',25)

n!Wuew+R$$Qhf^mbGR,2%tr@a
1 rows selected

4-9. Creating a Comma-Separated Values File
Problem
You need to export an Oracle table or view to CSV (Comma-Separated Values) for import into Microsoft Excel or another application that can import data in CSV format.

Solution
Use SQL*Plus, a custom query against one or more tables, and a set of carefully selected SQL*Plus commands to create a text file that will need no further processing before import into Microsoft Excel.


The query you use to retrieve the data returns the first line with the column names, then the rows in the table with commas separating each column and double quotes around each string. Here is the query along with the required SQL*Plus commands that you can store in a file called emp_sal.sql, for example:

-- suppress sql output in results set echo off
-- eliminate row count message at end set feedback off
-- make line long enough to hold all row data set linesize 1000
-- suppress headings and page breaks set pagesize 0
-- eliminate SQL*Plus prompts from output set sqlprompt ''
-- eliminate trailing blanks set trimspool on
-- send output to file spool emp_sal.csv
select '"EMPLOYEE_ID","LAST_NAME","FIRST_NAME","SALARY"' from dual
union all
select employee_id || ',"' || last_name || '","' || first_name || '",' || salary
from employees
;
spool off exit
The output from running this script looks like this:
C:\>sqlplus hr/hr@recipes @emp_sal.sql
SQL*Plus: Release 11.1.0.6.0 - Production on Sun Sep 27 22:12:01 2009 Copyright (c) 1982, 2007, Oracle. All rights reserved.

Connected to:
Oracle Database 11g Enterprise Edition Release 11.1.0.6.0 - Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
"EMPLOYEE_ID","LAST_NAME","FIRST_NAME","SALARY" 100,"King","Steven",21266.67
101,"Kochhar","Neena",44098.56 102,"De Haan","Lex",21266.67
. . .
203,"Mavris","Susan",7150 204,"Baer","Hermann",11000 205,"Higgins","Shelley",11165 206,"Gietz","William",11165


Disconnected from Oracle Database 11g Enterprise Edition Release 11.1.0.6.0 - Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
C:\>
How It Works
Many ETL (Extract, Transform, and Load) tools include an option to read from an Oracle database and create an output file in CSV format. If your data movement needs are rather modest and a six-figure ETL tool license is not within your budget, the SQL*Plus version may suffice. Even SQL*Developer, a free tool from Oracle, will export the results of a query or a table to a number of formats including Microsoft Excel, but SQL*Plus is more amenable to scripting and scheduling within a batch job.
The SQL*Plus commands ensure that the default column headers are off, the output width is sufficient, and so forth. The SPOOL and SPOOL OFF commands send the output to a file on disk that can be read by Microsoft Excel or another program that can read CSV files.
When you open the file emp_sal.csv generated in the recipe solution with Microsoft Excel, it looks much like any spreadsheet, as you can see in Figure 4-1.


Figure 4-1. A SQL*Plus-generated CSV file opened in Microsoft Excel




In this chapter we introduce recipes to handle many common query patterns, often for the sort of problems that you know should have an elegant solution, but for which no obvious SQL command or operator exists. Whether it’s predicting the trend in data for extrapolation and planning purposes, paginating your data for display on a web site, or finding lost text in your database, you’ll find a recipe to suit you in this chapter.
Many of the problems we address here have multiple solutions. Where space allows, we’ve tried to cover as many options as possible. Having said that, if you invent or know of a different approach, consider yourself a budding SQL chef too!

5-1. Changing Nulls into Real Values
Problem
You want to substitute null values in a table or result with a meaningful alternative.

Solution
Oracle provides several functions to allow manipulation and conversion of null values into arbitrary literals, including NVL, NVL2, COALESCE, and CASE-based tests. Each of these allows for different logic when handling null values, translating their inclusion in results in different ways. The goal is to turn NULL values into something more meaningful to the non-database professional, or more appropriate to the business context in which the data will be used.
Our recipe’s first SELECT statement uses the NVL function to return an employee’s id and surname,
and either the commission percentage or the human-readable “no commission” for staff with a null
COMMISSION_PCT.
select employee_id, last_name,
nvl(to_char(commission_pct), 'No Commission') as COMMISSION from hr.employees;



…
The second version of our recipe targets the same information, but uses the NVL2 function to split the employees into those that are paid a commission and those that aren’t.
select employee_id, last_name,

nvl2(commission_pct, 'Commission Based', 'No Commission') as COMMISSION from hr.employees;




…
Our third recipe returns the product of the COMMISSION_PCT and SALARY for commission-based employees, or the SALARY value for those with NULL COMMISSION_PCT, using the COALESCE function.
select employee_id, last_name,

coalesce((1 + commission_pct) * salary, salary) as SAL_INC_COMM from hr.employees;




…


Our fourth recipe uses the CASE feature to return the salary of non-commissioned employees.
select employee_id, last_name, case
when commission_pct is null then salary else (1 + commission_pct) * salary
end total_pay from hr.employees;
The results are the same as our recipe code using the COALESCE function.

How It Works
Each of the NVL, NVL2, and COALESCE functions let you handle NULL values in different ways, depending on requirements. Our recipe using NVL selects the EMPLOYEE_ID and LAST_NAME in basic fashion, and then uses NVL to identify whether COMMISSION_PCT has a value or is NULL. Those rows with a genuine value have that value returned, and those rows with NULL return the text “No Commission” instead of nothing at all.
The general structure of the NVL function takes this basic form.
nvl(expression, <value to return if expression is null>)
By contrast, the second recipe takes the NULL handling slightly further using the NVL2 function. Instead of just substituting a placeholder value for NULL, the NVL2 function acts as a switch. In our example, NVL2 evaluates the value of COMMISSION_PCT. If a real value is detected, the second expression is returned. If a NULL is detected by NVL2, the third expression is returned.
The general form of the NVL2 function is shown next.
nvl(expression,
<value to return if expression is not null>,
<value to return if expression is null>)
Third, we used the COALESCE function to find the first non-NULL value among COMMISSION_PCT and SALARY for each employee. The COALESCE function can take an arbitrary number of expressions, and will return the first expression that resolves to a non-NULL value. The general form is:
coalesce(expression_1, expression_2, expression_3 … expression_n)
Lastly, we use the CASE expression to evaluate the COMMISSION_PCT field and to switch logic if the value found is NULL. The general form of the CASE expression is:
case
when <expression> then <value, expression, column>
<optional additional when clauses>
else <some default value, expression, column> end


5-2. Sorting on Null Values
Problem
Results for a business report are sorted by department manager, but you need to find a way to override the sorting of nulls so they appear where you want at the beginning or end of the report.

Solution
Oracle provides two extensions to the ORDER BY clause to enable SQL developers to treat NULL values separately from the known data, allowing any NULL entries to sort explicitly to the beginning or end of the results.
For our recipe, we’ll assume that the report desired is based on the department names and manager identifiers from the HR.DEPARTMENTS table. This SQL selects this data and uses the NULLS FIRST option to explicitly control NULL handling.

select department_name, manager_id from hr.departments
order by manager_id nulls first;

The results present the “unmanaged” departments first, followed by the departments with managers by MANAGER_ID. We’ve abbreviated the results to save trees.



How It Works
Normally, Oracle sorts NULL values to the end of the results for default ascending sorts, and to the beginning of the results for descending sorts. The NULLS FIRST ORDER BY option, together with its complement, NULLS LAST, overrides Oracle’s normal sorting behavior for NULL values and places them exactly where you specify: either at the beginning or end of the results.
Your first instinct when presented with the problem of NULL values sorting to the “wrong” end of your data might be to simply switch from ascending to descending sort, or vice versa. But if you think about more complex queries, subselects using ROWNUM or ROWID tricks, and other queries that need to


preserve data order while getting NULL values moved, you’ll see that NULLS FIRST and NULLS LAST have real utility. Using them guarantees where the NULL values appear, regardless of how the data values are sorted.

5-3. Paginating Query Results
Problem
You need to display query results on web pages, showing a subset of results on each page. Users will be able to navigate back and forth through the pages of results.

Solution
Solving pagination problems requires thinking of the process in more generic terms—and using several fundamental Oracle features in combination to tackle problems like this. What we’re really attempting to do is find a defined subset of a set of results, whether we display this subset on a web page or report, or feed it in to some subsequent process. There is no need to alter your data to add explicit page numbers or partitioning details. Our solutions will use Oracle’s ROWNUM pseudo-column and ROW_NUMBER OLAP function to handle implicit page calculations, and nested subselects to control which page of data we see.
We’ll use the OE.PRODUCT_INFORMATION table as the source for our solution, supposing we’ll use the
data therein to publish an online shopping web site. In the sample schema Oracle provides, the OE.PRODUCT_INFORMATION table holds 288 rows. That’s too many for a single web page aesthetically, even though you could technically display a list that long. Your poor customers would tire of scrolling before they bought anything!
Thankfully, we can save our clients with the next SELECT statement. We’ll control our SELECT
statement to return 10 results only.

select product_id, product_name, list_price from (select prodinfo.*, rownum r
from
(select product_id, product_name, list_price from oe.product_information
order by product_id) prodinfo where rownum <= 10)
where r >= 1;
For once, we won’t abbreviate the results the solution so you can see we have 10 rows of results for display on our hypothetical web page.






An alternative technique is to use the ROW_NUMBER OLAP function to perform equivalent row numbering, and similarly filter by a predicate on the numbers it produces.

select product_id, product_name, list_price from (select product_id, product_name, list_price,
row_number() over (order by product_id) r from oe.product_information)
where r between 1 and 10
The results are the same, including the historical artifacts like 18 gigabyte hard disk drives for $800!
Those were the days.

How It Works
Let’s examine the ROWNUM technique first. The core of this solution is using ROWNUM in a subselect to tag the data we really want with a handy number that controls the pagination. The ROWNUM values aren’t stored anywhere: they’re not a column in your table or hidden somewhere else. ROWNUM is a pseudo-column of your results, and it only comes into being when your results are gathered, but before any sorting or aggregation.
Looking at the subselects at the heart of our recipe, you’ll see the format has this structure.
select <columns I actually want>, rownum r
from
(select <columns I actually want> from oe.product_information
order by product_id) where rownum <= 10

We select the actual data and columns we want and wrap those in a similar SELECT that adds the ROWNUM value. We give this an alias for later use in the outer SQL statement, which we’ll explain shortly. The FROM and ORDER BY clauses are self-explanatory, leaving us the ROWNUM predicate. In this case, we look for ROWNUM values less than 10, because ultimately we want to show the rows from 1 to 10. In its general form, we are really asking for all matching result rows with a ROWNUM up to the end point we want for our page. So if we wanted to show items 41 to 50 on page 5 of our hypothetical web site, this clause would read WHERE ROWNUM <= 50. In your application code or stored procedure, it’s natural to replace this with a bind variable.
where rownum <= :page-end-row

This means the subselect will actually produce results for all rows up to the last row you intend to use in pagination. If we use our query to ultimately display the page for items 41 to 50 with the WHERE


ROWNUM <= 50 option, the subselect will gather the results for 50 rows, not just the 10 you intend to display. At this point, the outer query comes in to play.
The structure of the outer query has this general form:
select <columns I actually want> from
(<rows upto the end-point of pagination provided by subselect>) where r >= 1;

We’re performing a quite normal subselect, where we take the meaningful columns from the subselect for display in the SELECT portion of the statement, and use our WHERE predicate to lop off any unnecessary leading rows from the results of the subselect, leaving us with the perfect set of data for pagination. For our recipe modification targeting rows 41 to 50, this predicate would change to WHERE R
>= 41. Again, using this in a prepared fashion or through a stored procedure would typically be done with a bind variable, giving this general form of the WHERE clause.
where r >= :page-start-row

At this point, we hope you find yourself with two lingering questions. Why did we introduce the column alias “r” for ROWNUM in the subselect rather than just using the ROWNUM feature again in the outer SELECT statement, and why didn’t we just use a BETWEEN clause to simplify the whole design? The alias is used to preserve the ROWNUM values from the subselect for use in the outer select. Suppose instead we’d tried to use ROWNUM again as in the next SQL statement.

-- Note! Intentionally flawed rownum logic select product_id, product_name, list_price from (select prodinfo.*, rownum from
(select product_id, product_name, list_price from oe.product_information
order by product_id) prodinfo where rownum <= 50)
where rownum >= 41;

Try running that yourself and you’ll be surprised to find no rows returned. Equally, if we stripped out the subselect constructs and just tried a BETWEEN clause, we’d construct SQL like the next example.

-- Note! Intentionally flawed rownum logic select product_id, product_name, list_price from oe.product_information
where rownum between 41 and 50;
Oops! Once again, no rows returned. Why are these failing? It’s because the ROWNUM mechanism only assigns a value after the basic (non-sorting/aggregating) criteria are satisfied, and always starts with a ROWNUM value of 1. Only after that value is assigned does it increment to 2, and so on. By removing the alias, we end up generating new ROWNUM values in the outer SELECT and then asking if the first candidate value, 1, is greater than or equal to 41. It’s not, so that row is discarded and our ROWNUM value doesn’t increment. No subsequent rows satisfy the same predicate—and you end up with no results. Using the BETWEEN variant introduces the same problem. That’s why we use the alias in the first place, and why we refer to the preserved “r” values in our successful recipe.




Caution Developers often fail to test non-obvious cases when using this style of logic. There is one subset of data where our problematic examples would return results. This would be for the first page of data, where the
page starting row does have a ROWNUM value of 1. This is a classic case of the boundary condition working but all
other possible data ranges failing. Make sure your testing covers both the natural end points of your data, as well as ranges in between, to save yourself from such pitfalls.


Our second recipe takes a different tack. Using the ROW_NUMBER OLAP function, we achieve the same outcome in one pass over the data. Like all OLAP functions, the ROW_NUMBER function is applied to the query after the normal predicates, joins, and the like are evaluated. For the purposes of pagination, this means that the subquery effectively queries all of the data from the OE.PRODUCT_INFORMATION table, and ROW_NUMBER then applies an incrementing number by the order indicated, in this case by PRODUCT_ID.

select product_id, product_name, list_price, row_number() over (order by product_id) r from oe.product_information
If we evaluated the subquery by itself, we’d see results like this (abbreviated to save paper).
PRODUCT_ID	PRODUCT_NAME	LIST_PRICE	R

288 rows selected.
Every row of the table is returned, as our SELECT statement hasn’t qualified the data with any filtering predicates. The ROW_NUMBER function has numbered all 288 rows, from 1 to 288.
At this point, the outer query comes into play and its role is very straightforward. Our recipe uses an outer query statement that looks like the next SQL snippet.
select product_id, product_name, list_price from
(<subselect returning desired columns and row number “r”>) where r between 1 and 10

This outer SQL is as obvious as it looks. Select the desired columns returned from the subselect, where the r value (generated by the subselect’s ROW_NUMBER function) is between 1 and 10. In comparison to our other solution that used ROWNUM, if we wanted to represent a different page of results for products 41 to 50, the outer query’s WHERE clause would simply change to WHERE R BETWEEN 41 AND 50. In the general form, we’d recommend parameterizing the page-start-row and page-end-row, as illustrated in the following SQL pseudo code.
select <desired columns>


(select <desired columns>,
row_number() over (order by <ordering column>) r from <source table, view, etc.>)
where r between :page-start-row and :page-end-row
By now you’re probably asking yourself which of the two methods, ROWNUM and ROW_NUMBER, you should use. There’s no clear-cut answer to that question. Instead, it’s best to remember some of the qualities of both recipes. The ROWNUM approach enjoys historical support and has built-in optimization in Oracle to make the sort induced by the ordering faster than a normal sort. The ROW_NUMBER approach is more versatile, enabling you to number your rows in an order different from the order produced by a standard ORDER BY clause. You can also alter the recipe to use other OLAP functions like RANK and DENSE_RANK to handle different paging requirements, such as needing to show items in “tied” positions.


Our recipe drives the pagination of data where it is generally best handled—in the database! You may have used or experienced other techniques, such as cursor-driven pagination and even result-caching at the application tier, with data discarded or hidden to provide pagination.
Our advice is to eschew those techniques in pretty much all cases. The application caching technique inevitably requires excessive network traffic with associated performance delay and cost, which the end user won’t appreciate. Cursor-driven approaches do work at the database tier to minimize the network issue, but you are always susceptible to “stale” pagination with cursor techniques. Don’t be in any doubt: use the database to paginate for you—and reap the benefits!

5-4. Testing for the Existence of Data
Problem
You would like to compare the data in two related tables, to show where matching data exists, and to also show where matching data doesn’t exist.

Solution
Oracle supports the EXISTS and NOT EXISTS predicates, allowing you to correlate the data in one table or expression with matching or missing data in another table or expression. We’ll use the hypothetical situation of needing to find which departments currently have managers. Phrasing this in a way that best illustrates the EXISTS solution, the next SQL statement finds all departments where a manager is known to exist.

select department_name from hr.departments d where exists
(select e.employee_id


from hr.employees e
where d.manager_id = e.employee_id);

The complement, testing for non-existence, is shown in the next statement. We ask to find all departments in HR.DEPARTMENTS with a manager that does not exist in the data held in HR.EMPLOYEES.

select department_name from hr.departments d where not exists (select e.employee_id from hr.employees e
where d.manager_id = e.employee_id);

How It Works
In any database, including Oracle, the EXISTS predicate answers the question, “Is there a relationship between two data items, and by extension, what items in one set are related to items in a second set?” The NOT EXISTS variant tests the converse, “Can it definitively be said that no relationship exists between two sets of data, based on a proposed criterion?” Each approach is referred to as correlation or a correlated subquery (literally, co-relation, sharing a relationship).
Interestingly, Oracle bases its decision on whether satisfying data exists solely on this premise: was a matching row found that satisfied the subquery’s predicates? It’s almost too subtle, so we’ll point out the obvious thing Oracle isn’t seeking. What you select in the inner correlated query doesn’t matter—it’s only the criteria that matter. So you’ll often see versions of existence tests that form their subselect by selecting the value 1, the entire row using an asterisk, a literal value, or even NULL. Ultimately, it’s immaterial in this form of the recipe. The key point is the correlation expression. In our case, it’s WHERE D.MANAGER_ID = E.EMPLOYEE_ID.
This also helps explain what Oracle is doing in the second half of the recipe, where we’re looking for DEPARTMENT_NAME values for rows where the MANAGER_ID doesn’t exist in the HR.EMPLOYEES table. Oracle drives the query by evaluating, for each row in the outer query, whether no rows are returned by the inner correlated query. Oracle doesn’t care what data exists in other columns not in the correlation criteria. It pays to be careful using such NOT EXISTS clauses on their own—not because the logic won’t work but because against large data sets, the optimizer can decided to repeatedly scan the inner data in full, which might affect performance. In our example, so long as a manager’s ID listed for a department is not found in the HR.EMPLOYEES table, the NOT EXISTS predicate will be satisfied, and department included in the results.


Caution Correlated subqueries satisfy an important problem-solving niche, but it’s crucial to remember the nature of NULL values when using either EXISTS or NOT EXISTS. NULL values are not equivalent to any other value, including other NULL values. This means that a NULL in the outer part of a correlated query will never satisfy the
correlation criterion for the inner table, view, or expression. In practice, this means you’ll see precisely the opposite effect as the one you might expect because the EXISTS test will always return false, even if both the inner and outer data sources have NULL values for the correlation, and NOT EXISTS will always return true. Not
what the lay person would expect.




5-5. Conditional Branching In One SQL Statement
Problem
In order to produce a concise result in one query, you need to change the column returned on a row-by- row basis, conditional on a value from another row. You want to avoid awkward mixes of unions, subqueries, aggregation, and other inelegant techniques.

Solution
For circumstances where you need to conditionally branch or alternate between source data, Oracle provides the CASE statement. CASE mimics the traditional switch or case statement found in many programming languages like C or Java.
To bring focus to our example, we’ll assume our problem is far more tangible and straightforward. We want to find the date employees in the shipping department (with the DEPARTMENT_ID of 50) started their current job. We know their initial hire date with the firm is tracked in the HIRE_DATE column on the HR.EMPLOYEES table, but if they’ve had a promotion or changed roles, the date when they commenced their new position can be inferred from the END_DATE of their previous position in the HR.JOB_HISTORY table. We need to branch between HIRE_DATE or END_DATE for each employee of the shipping department accordingly, as shown in the next SQL statement.

select e.employee_id, case
when old.job_id is null then e.hire_date else old.end_date end
job_start_date
from hr.employees e left outer join hr.job_history old on e.employee_id = old.employee_id
where e.department_id = 50 order by e.employee_id;
Our results are very straightforward, hiding the complexity that went into picking the correct

JOB_START_DATE.




…

How It Works
Our recipe uses the CASE feature, in Search form rather than Simple form, to switch between HIRE_DATE
and END_DATE values from the joined tables. In some respects, it’s easiest to think of this CASE operation


as a combination of two SELECT statements in one. For employees with no promotions, it’s as if we were selecting as follows:
select e.employee_id, e.hire_date…
Whereas for employees that have had promotions, the CASE statement switches the SELECT to the following form:
select e.employee_id, old.end_date…

The beauty is in not having to explicitly code these statements yourself, and for far more complex uses of CASE, not having to code many dozens or hundreds of statement combinations.
To explore the solution from the data’s perspective, the following SQL statement extracts the employee identifier and the hire and end dates using the same left outer join as our recipe.

select e.employee_id, e.hire_date, old.end_date end from hr.employees e left outer join hr.job_history old on e.employee_id = old.employee_id

where e.department_id = 50 order by e.employee_id;



…
The results show the data that drove the CASE function’s decision in our recipe. The values in bold were the results returned by our recipe. For the first, second, fourth, and fifth rows shown, END_DATE from the HR.JOB_HISTORY table is NULL, so the CASE operation returned the HIRE_DATE. For the third row, with EMPLOYEE_ID 122, END_DATE has a date value, and thus was returned in preference to HIRE_DATE when examined by our original recipe. There is a shorthand form of the CASE statement known as the Simple CASE that only operates against one column or expression and has THEN clauses for possible values. This wouldn’t have suited us as Oracle limits the use of NULL with the Simple CASE in awkward ways.

5-6. Conditional Sorting and Sorting By Function
Problem
While querying some data, you need to sort by an optional value, and where that value is not present, you’d like to change the sorting condition to another column.


Solution
Oracle supports the use of almost all of its expressions and functions in the ORDER BY clause. This includes the ability to use the CASE statement and simple and complex functions like arithmetic operators to dynamically control ordering. For our recipe, we’ll tackle a situation where we want to show employees ordered by highest-paid to lowest-paid.
For those with a commission, we want to assume the commission is earned but don’t want to actually calculate and show this value; we simply want to order on the implied result. The following SQL leverages the CASE statement in the ORDER BY clause to conditionally branch sorting logic for those with and without a COMMISSION_PCT value.

select employee_id, last_name, salary, commission_pct from hr.employees
order by case
when commission_pct is null then salary else salary * (1+commission_pct)
end desc;

We can see from just the first few rows of results how the conditional branching while sorting has worked.




…
Even though employees 101 and 102 have a higher base salary, the ORDER BY clause using CASE has correctly positioned employees 145 and 146 based on their included commission percentage.

How It Works
The selection of data for our results follows Oracle’s normal approach, so employee identifiers, last names, and so forth are fetched from the HR.EMPLOYEES table. For the purposes of ordering the data using our CASE expression, Oracle performs additional calculations that aren’t shown in the results. All the candidate result rows that have a non-NULL commission value have the product of COMMISSION_PCT and SALARY calculated and then used to compare with the SALARY figures for all other employees for ordering purposes.
The next SELECT statement helps you visualize the data Oracle is deriving for the ordering calculation.

select employee_id, last_name, commission_pct, salary, salary * (1+commission_pct) sal_x_comm
from hr.employees;



…

The values in bold show the calculations Oracle used for ordering when evaluating the data via the CASE expression in the ORDER BY clause. The general form of the CASE expression can be expressed simply as follows.
case
when <expression, column, etc.> then <expression, column, literal, etc.> when <expression, column, etc.> then <expression, column, literal, etc.>
…
else <default expression for unmatched cases> end

We won’t needlessly repeat what the Oracle manual covers in plenty of detail. In short, the CASE expression evaluates the first WHEN clause for a match and if satisfied, performs the THEN expression. If the first WHEN clause isn’t satisfied, it tries the second WHEN clause, and so on. If no matches are found, the ELSE default expression is evaluated.

5-7. Overcoming Issues and Errors when Subselects Return Unexpected Multiple Values
Problem
In working with data from a subselect, you need to deal with ambiguous situations where in some cases the subselect will return a single (scalar) value, and in other cases multiple values.

Solution
Oracle supports three expressions that allow a subselect to be compared based on a single column of results. The operators ANY, SOME, and ALL allow one or more single-column values from a subselect to be compared to data in an outer SELECT. Using these operators allows you to deal with situations where you’d like to code your SQL to handle comparisons with flexible set sizes.
Our recipe focuses on using these expressions for a concrete business problem. The order-entry system tracks product information in the OE.PRODUCT_INFORMATION table, including the LIST_PRICE value. However, we know discounts are often offered, so we’d like to get an approximate idea of which items have never sold at full price. To do this, we could do a precise correlated subquery of every sale against list price. Before doing that, a very quick approximation can be done to see if any LIST_PRICE value is


higher than any known sale price for any item, indicated by the UNIT_PRICE column of the
OE.ORDER_ITEMS table. Our SELECT statement takes this form.
select product_id, product_name from oe.product_information where list_price > ALL
(select unit_price from oe.order_items);
From this query, we see three results:
PRODUCT_ID	PRODUCT_NAME
2351	Desk - W/48/R
3003	Laptop 128/12/56/v90/110
2779	Desk - OS/O/F
These results mean at least three items—two desks and a laptop—have never sold a full price.

How It Works
Our example’s use of the ALL expression allows Oracle to compare the UNIT_PRICE values from every sale, to see if any known sale price was greater than the LIST_PRICE for an item. Breaking down the steps that make this approach useful, we can first look at the subselect.

select unit_price from oe.order_items

This is a very simple statement that returns a single-column result set of zero or more items, shown next in abbreviated form.
UNIT_PRICE
13
38
43
43
482.9
…
665 rows selected.
Based on those results, our outer SELECT statement compares each LIST_PRICE from the OE.PRODUCTION_INFORMATION table with every item in the list, as we are using the ALL expression. If the LIST_PRICE is greater than all of the returned values, the expression resolves to true, and the product is included in the results. Where even one of the UNIT_PRICE values returned exceeds the LIST_PRICE of an item, the expression is false and that row is discarded from further consideration.
If you review that logic, you’ll realize this is not a precise calculation of every item that didn’t sell for full price. Rather, it’s just a quick approximation, though one that shows off the ALL technique quite well.
The alternatives SOME and ANY, which are effectively synonyms, resolve the true/false determination based on only needing one item in the subselect to satisfy the SOME/ANY condition. Oracle will happily


accept more than one item matching the SOME/ANY criteria, but only needs to determine one value to evaluate the subselect.

5-8. Converting Numbers Between Different Bases
Problem
You need to find a generic way to turn decimal representations of numbers such as IP addresses and MAC addresses into hexadecimal, octal, binary, and other unusual number bases.

Solution
Oracle ships with a small number of base-conversion features, the most notable of which is the TO_CHAR function’s ability to convert from decimal to hexadecimal. We can convert an arbitrary value, such as 19452, to hexadecimal using the next SELECT statement:

select to_char(19452,'xxxxx')-oi-0ij from dual;

Our output shows the correctly calculated value of 4BFC. Run that query yourself to confirm Oracle’s accuracy at hexadecimal conversion. While this is useful for this specific case, often we’ll want to see decimals converted to binary, octal, and other uncommon bases.


Tip You may ask yourself, what other bases might possibly be used? One of the authors once worked on software that stored all numbers in base 36, using the digits 0 to 9 and the letters A to Z to deal with limitations in certain hardware products that could only store strings. You may find yourself needing number conversion to unusual bases when you least expect it!


Other base conversions are not natively addressed out of the box by Oracle, so we’ll create our own generic decimal-conversion function that can take a given number and convert it to any base from 2 to 36! We’ll build our REBASE_NUMBER function using the fundamental arithmetic operations that do ship with Oracle, as shown here.

create or replace function rebase_number (starting_value in integer, new_base in number) return varchar2
is
rebased_value varchar2(4000) default NULL; working_remainder integer default starting_value;
char_string varchar2(36) default '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'; sign varchar2(1) default '';
begin


if (starting_value < 0) then sign := '-';
working_remainder := abs(working_remainder); end if;
loop
rebased_value := substr(char_string, mod(working_remainder,new_base)+1, 1)
|| rebased_value;
working_remainder := trunc(working_remainder/new_base); exit when (working_remainder = 0);
end loop;
rebased_value := sign || rebased_value; return rebased_value;
end rebase_number;
/
With the REBASE_NUMBER function now available, we can perform conversions of our original test number, 19452, to a variety of bases. The first example shows conversion to hexadecimal, to prove we’re getting the same result as Oracle.

select rebase_number(19452,16) as DEC_TO_HEX from dual;
We successfully calculate the correct result.
DEC_TO_HEX
4BFC
The same number converted to octal also succeeds.

select rebase_number(19452,8) as DEC_TO_OCTAL from dual;
DEC_TO_OCTAL
45774
A final example converting to binary similarly produces the correct result.

select rebase_number(19452,2) as DEC_TO_BINARY from dual;
DEC_TO_BINARY
100101111111100


How It Works
Our function models the classic discrete mathematics used to perform a conversion between two bases for any number. The basic algorithm takes this form: For each significant digit in the original number (thousands, hundreds, and so forth)
Perform modulo division on that part of the number using the new base.
Use the whole part of the result as the new “digit” in the converted number.
Take the remainder, and repeat until the remainder is less than the new base. Return the new digits and the last remainder as the converted number.
There are two not-so-obvious aspects of our implementation of this logic that warrant further explanation. The first involves dealing with negative numbers. Our function sets up various local variables, some of which we’ll discuss shortly, the last of which is called SIGN. We set this to an empty string, which implicitly means a positive outcome reported to the caller unless we detect that a negative number is passed to the function for conversion. In the first part of the logic body of our function, we test for the sign of the STARTING_VALUE and change the SIGN value to '-' for any negative values detected.

if (starting_value < 0) then sign := '-';
working_remainder := abs(working_remainder); end if;

Detecting negative numbers and switching the WORKING_NUMBER to positive using the ABS function isn’t part of normal decimal-to-hexadecimal conversion. In fact, we don’t use it in our arithmetic either! It exists purely to help with the second non-obvious aspect of our implementation, a trick we utilize in our function that we’ll cover in a moment. At the end of our function, we reinstate the SIGN as it should be, so that the user doesn’t notice this data massaging happening.
rebased_value := sign || rebased_value;
So why the two-step with the number’s sign in the first place? It’s because we use a trick to “look up” the conversion of a decimal digit (or digits) into its new base by finding how the whole part of our modulo division maps to a position in a string. In this case, the string is the unusual local variable you see at the start of the function, CHAR_STRING.
char_string varchar2(36) default '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ';

You may have thought, why are the authors practicing their typing in this book? But we’re not! The trick is to use the SUBSTR function to walk this string to find the matching value for our conversion. Here’s how it works at its basic level. We take a source number, like 13, and the desired base, like 16, and plug it into our SUBSTR call, as shown in the next fragment.
substr(char_string, mod(13 /* our remainder */, 16 /* our new base */)+1, 1)
Performing the arithmetic is easy. 13 modulo 16 gives 0 with a remainder of 13. We add one to this value, because our string is zero-based, so we need to account for the “zeroth” position, giving us a value of 14. We then take the substring of the CHAR_STRING starting at this position (14), for 1 character. The 14th


character in the string is D. Voilà: we have converted the decimal number 13 to its hexadecimal equivalent, D. From there, it’s just a case of repeating the process for successive modulo division results.
The reverse logic can also be incorporated into a function, so values in any base can be converted to decimal. We can also wrap these functions in a helper function that would allow the conversion from any arbitrary base to any other arbitrary base. These functions are available at www.oraclesqlrecipes.com.

5-9. Searching for a String Without Knowing the Column or Table
Problem
You need to find where in the database a particular text value is stored, but don’t have access to the application code or logic that would let you determine this from seeing the data via the application. You have only the Oracle catalog tables and SQL to find what you seek.

Solution
No database currently available includes an omniscient “search everywhere for what I’m seeking” operator. However, using the basic building blocks of Oracle’s string-searching LIKE operator, and the ability to output one SELECT statement as the result of another, we can execute a series of steps that will automatically track down the schema, table, and column holding the text data we seek.
To give our recipe a concrete flavor, we’ll assume that we’re looking for the text “Greene” somewhere in the database, but we don’t know—and can’t find any documentation that would tell us— if this text is a person’s name, a supplier or client company name, part of a product name, or part of some other text.
Our solution works in two parts. The first SELECT statement, shown next, produces as results individual SELECT queries to search all columns of all tables in all schemata for our unidentified text, Greene.
Select
'select ''' || owner || ''',
''' || table_name || ''',
''' || column_name || ''', ' || column_name ||
' from ' || owner || '.' ||
table_name || ' where ' ||
column_name || '
like ''%Greene%'';' as Child_Select_Statements from all_tab_columns
where owner in ('BI', 'HR', 'IX', 'OE', 'PM', 'SH')
and data_type in ('VARCHAR2','CHAR','NVARCHAR2','NCHAR')
and data_length >= length('Greene');


In our example, the output of this command is a list of 271 SELECT statements, one for each text-like column in each of the tables in Oracle’s sample schemata. A subset is shown here, with formatting to make the output readable on this printed page.
CHILD_SELECT_STATEMENTS
…
select 'HR','COUNTRIES','COUNTRY_NAME', COUNTRY_NAME from HR.COUNTRIES
where COUNTRY_NAME like '%Greene%';
select 'HR','DEPARTMENTS','DEPARTMENT_NAME', DEPARTMENT_NAME from HR.DEPARTMENTS
where DEPARTMENT_NAME like '%Greene%';
select 'HR','EMPLOYEES','FIRST_NAME', FIRST_NAME from HR.EMPLOYEES
where FIRST_NAME like '%Greene%';
select 'HR','EMPLOYEES','LAST_NAME', LAST_NAME from HR.EMPLOYEES
where LAST_NAME like '%Greene%'; select 'HR','EMPLOYEES','EMAIL', EMAIL from HR.EMPLOYEES
where EMAIL like '%Greene%';
…

As you’re looking at these output statements, first stop and think if there are performance implications to running statements across many (if not all) of your tables. If your database is multi- terabyte in size, it might be worth some planning to execute these when you won’t affect performance. Once you’re happy with the logistics of when it’s best to use them, run them against the database. They will elicit the schema, table, and column name, plus the column data, where the sought-after text resides. A portion of the results are shown next.

select 'HR','EMPLOYEES','FIRST_NAME', FIRST_NAME from HR.EMPLOYEES
where FIRST_NAME like '%Greene%';
no rows selected

select 'HR','EMPLOYEES','LAST_NAME', LAST_NAME from HR.EMPLOYEES
where LAST_NAME like '%Greene%';
'H 'EMPLOYEE 'LAST_NAM LAST_NAME
HR EMPLOYEES LAST_NAME Greene


select 'HR','EMPLOYEES','EMAIL', EMAIL from HR.EMPLOYEES
where EMAIL like '%Greene%';
no rows selected
With the completion of the second set of statements, we’ve found the text “Greene” in the LAST_NAME
column of the HR.EMPLOYEES table.

How It Works
This solution takes a two-pass approach to the problem. When you run the first query, it searches for all the columns in the ALL_TAB_COLUMNS system view to find the schema names, table names, and column names for columns that have a textual data type, such as VARCHAR2 or CHAR. It’s a little hard to see the logic through all the literal formatting and string concatenation, so it’s best thought of using this general structure.
select
<escaped column, table and schema names for later presentation>,
<actual column name for later querying>,
<escaped from clause for later querying >
<escaped where clause for later querying > from all_tab_columns
where owner in (<list of schemata in which we’re interested>) and data_type in ('VARCHAR2','CHAR','NVARCHAR2','NCHAR')
and data_length >= <length of text sought>;

For each table with at least one relevant textual data type, this query will emit a result that takes the form of a SELECT statement that generally looks like this:
select
<literal schema name>,
<literal table name>,
<literal column name>,
<column_name>
where <column_name> like '%<text sought>%';
It’s then just a matter of running those SELECT statements and noting which ones actually produce results. All of the elaborate literal escaping and quoting is used to preserve the object names right down to this level, so the results include not only the text you seek in context, but also the human-readable data for schema, table, and column, as you can see in this example row.
HR EMPLOYEES LAST_NAME Greene
At least one instance of the text “Greene” can be found in HR.EMPLOYEES, in the LAST_NAME column.


 	ALL_TAB_COLUMNS versus ALL_TAB_COLS	

Careful observers will note that we’ve crafted our solution to work with the ALL_TAB_COLUMNS system view. Oracle also includes a system view named ALL_TAB_COLS. The two seem almost identical, having the same fields, but slightly different row counts. So why choose ALL_TAB_COLUMNS in this recipe?
The answer has to do with the slightly different definitions Oracle uses in defining the two views. The ALL_TAB_COLS system view includes hidden columns not normally visible to users or developers. As our recipe seeks ordinary data used every day by end-users, we rightly assume that the developers have not played tricks by either hiding columns of their own or abusing Oracle’s system-controlled hidden columns.
The recipe so far has not made any assumptions about the tool or tools you’ll use when working with the solution. You can happily run the first statement in SQL*Plus, SQL Developer, or through a programmable API, and retrieve the second set of statements to run. Similarly, you can use any tool to then execute those statements and view the results revealing the hiding place of your lost text. But some extra niceties are possible. For instance, you could wrap much of the logic in a PL/SQL function or stored procedure, or use the formatting capabilities of a query tool like SQL*Plus to make the solution even more elegant.

5-10. Predicting Data Values and Trends Beyond a Series End
Problem
From a large set of data, you need to predict the behavior or trend of the information beyond the bounds of your current data set.

Solution
Predicting trends and extrapolating possibilities based on patterns in the data, such as relationships between two values, can be addressed in many ways. One of the most popular techniques is linear regression analysis, and Oracle provides numerous linear regression functions for many contemporary trending and analysis algorithms.
As part of the sample schemata included with the Oracle database, a Sales History data warehouse example is provided in the SH schema. This includes a fact table of around a million entries of individual items sold with dimensions for time, sales channel, and so on, and the item price for a given sale.
Our recipe supposes that we’re solving the problem of predicting what would happen if we introduced more expensive items than those currently sold. In effect, we’d like to extrapolate beyond the current most expensive item, based on the current sales trend of volume sold compared with item price. We’re answering the fundamental question, would we actually sell an item if it were more expensive, and if so, how many of those items would we sell?
The following SELECT statement uses three Oracle linear regression functions to help guide our
extrapolation. We’re asking what’s likely to happen if we start selling items in the Electronics category at higher prices, will we sell more or fewer, and how quickly will sales volume change with price.


select
s.channel_desc,
regr_intercept(s.total_sold, p.prod_list_price) total_sold_intercept, regr_slope (s.total_sold, p.prod_list_price) trend_slope, regr_r2(s.total_sold, p.prod_list_price) r_squared_confidence
from sh.products p,
(select c.channel_desc, s.prod_id, s.time_id, sum(s.quantity_sold) total_sold from sh.sales s inner join sh.channels c
on s.channel_id = c.channel_id
group by c.channel_desc, s.prod_id, s.time_id) s where s.prod_id=p.prod_id
and p.prod_category='Electronics'
and s.time_id between to_date('01-JAN-1998') and to_date('31-DEC-1999') group by s.channel_desc
order by 1;
CHANNEL_DESC	TOTAL_SOLD_INTERCEPT	TREND_SLOPE	R_SQUARED_CONFIDENCE


For those unfamiliar with interpreting linear regressions, the extrapolated trends can be read in the general form:
Y = Intercept + Slope(X)

For our data, Y is the total number of items sold and X is the list price. We can see that for Direct Sales, Internet, and Partners, there’s a gentle decrease in sales as price increases, whereas sales volume plummets dramatically for Tele Sales as cost increases. However, our confidence values suggest the first three predictions are a poorly fit extrapolation for these channels. The R-squared confidence value indicates how well the extrapolated line of fit suits the data (also known as goodness of fit), where a value of 1.0 means “perfect fit”, and 0 means “absolutely no correlation”.

How It Works
To feed data to our linear regression functions, such as REGR_SLOPE, we need to ensure we are providing the actual values we want to compare. Our recipe compares the total number of items sold to the list price of an item. The SH.SALES table tracks the sale of each individual item, with one entry per item sold. It’s for this reason that we use the inline view to aggregate all of these individual sale entries into aggregate number of sales for a certain product, date, and channel. You can run the subselect by itself, as shown in the next SQL statement.

select c.channel_desc, s.prod_id, s.time_id, sum(s.quantity_sold) total_sold from sh.sales s inner join sh.channels c
on s.channel_id = c.channel_id
group by c.channel_desc, s.prod_id, s.time_id;


No surprises there, and the results provide a simple rolled-up summary ready for use in our linear regression functions.
CHANNEL_DESC	PROD_ID	TIME_ID	TOTAL_SOLD


Our results provide this summary for every type of item we sell. We join the inline view to the SH.PRODUCTS table, and then we filter by PROD_CATEGORY of Electronics and a two-year date range to focus on our supposed problem. The fun then starts with the linear regression functions in the SELECT clause.
Each of the three statistical functions takes the calculated TOTAL_SOLD amounts and LIST_PRICE
values for each channel (thanks to the GROUP BY clause), and performs the relevant calculations. A good statistics textbook will tell you how these formulae were derived, but Table 5-1 shows you each formula’s method.

Table 5-1. Oracle Statistical Methods Supporting Linear Regression Analysis
Function	Formula

REGR_INTERCEPT	(Σy)/n - (Σxy - (Σx)(Σy)/n)/(Σx² - (Σx)²/n) (Σx)/n

REGR_SLOPE	(Σxy - (Σx)(Σy)/n)/(Σx² - (Σx)²/n)
REGR_R2	(Σxy - (Σx)(Σy)/n)²/(Σx² - (Σx)²/n)(Σy² - (Σy)²/n)

Just looking at those formulae should make you glad you don’t have to code the calculations yourself. Just call the appropriate function and Oracle does the hard work for you. Armed with the data returned by those functions, you can then visualize how the sales volume changes with price. Figure 5-1 shows our linear regression data expressed as hypothetical extrapolated lines showing the relationship between sales volume and list price.




35

30

25
Direct Sales
Internet
20	Partners
Tele Sales
15

10

5

0
0	50	100	150	200	250
List Price (x)

Figure 5-1. An extrapolation of sales volume and list price using linear regression

This gives you a clear indication of how your sales volume might change as you alter price. Missing from this is an indication of goodness of fit: literally, how reliable is the estimated relationship between the data and the extrapolation taken from it. This is the r-squared value calculated using the REGR_R2 function.

5-11. Explicitly (Pessimistically) Locking Rows for an Update
Problem
To protect a particularly sensitive and special business case, you have been asked to ensure that updates to employee salaries are protected from lost updates, and also asked not to use an optimistic locking approach.

Solution
Our problem models one of the classic issues faced by developers and DBAs. While Oracle provides excellent concurrency control and locking mechanisms and supports both optimistic and pessimistic design approaches, often a political or business decision forces you to use one technique.


At some stage in your work with Oracle, you may find your most coherent arguments about Oracle’s excellent concurrency technology brushed aside by a manager who says “That’s all well and good, but I want to grant pay rises once, and I don’t want to see any messages indicating another manager updated the same employee’s salary and I should try again.”
Faced with such a career-threatening imperative, you’ll be glad to know Oracle provides the SELECT
… FOR UPDATE option to explicitly lock data and provide pessimistic concurrency control. For our recipe, we’ll use the next SELECT statement to find the salary data for employee 200, Jennifer Whalen, in preparation for a later update.

select employee_id, last_name, salary from hr.employees
where employee_id = 200 for update;



So far, so good. If a second user (or the same user via a different connection) attempts to perform a SELECT, UPDATE, or DELETE of this row, that user will be blocked. Try issuing the same SELECT statement from another connection and you’ll see no results as the session waits for the locks related to the SELECT
… FOR UPDATE pessimistic locking to be released.
select employee_id, last_name, salary from hr.employees
where employee_id = 200 for update;
(no results yet … waiting on blocking exclusive lock)
From the first session, we complete our update.
update hr.employees
set salary = salary * 1.1 where employee_id = 200;
commit;
Upon commit from the first session, the second session’s SELECT statement finally runs, returning these results.


Note the second session did not interrupt the update, and never saw the pre-updated data.


How It Works
The key to the pessimistic locking effect of the FOR UPDATE clause is in the way it forces Oracle to take locks on the data covered by the SELECT query. Oracle attempts to take a mode X (exclusive) transaction row-level lock on each row that satisfies a SELECT … FOR UPDATE query.
When the first connection issues the SELECT … FOR UPDATE statement, we can query the V$LOCK
dynamic view or use a graphical tool like SQL Developer to see the lock taken, as in Figure 5-2.


Figure 5-2. Select for update explicit locks for first session in SQL Developer

The highlighted row shows the lock on the row of the HR.EMPLOYEES table for EMPLOYEE_ID 200. As soon as the second connection attempts the same statement, it tries to acquire the same exclusive lock, even though the first consequence of doing so would normally only be to select the data. Figure 5-3 shows how Oracle’s normal “writes don’t block reads” behavior has been overridden by the FOR UPDATE technique.




Figure 5-3. The second select for update blocked, waiting for session 1

The highlighted row shows the second session in a WAIT state, blocked from taking any action until the first session commits or rolls back its work. As soon as we perform our update and commit the results from session 1, our blocking condition clears and the second session takes the exclusive row-level transaction lock it wanted, and can in turn perform its desired update. Figure 5-4 shows the cleared WAIT state and the second session now holding the lock.




Figure 5-4. Cleared from the wait state, the second select for update now holds an exclusive lock

In this way, you can take explicit pessimistic control of locking in Oracle in those specific circumstances where you can’t avoid it.


Caution While this approach might keep your boss happy, you might want to gently educate him or her over time to the great technology in Oracle, especially the non-blocking capabilities where readers don’t block writers and everyone gets a consistent view of the data. They may not want to hear it the first time, but keep dropping it into conversations using the phrase, “You were so clever to buy a database that can do this, Boss.” That way, their ego starts to work for you rather than against you, and you can avoid these pessimistic locking techniques as much as possible.




5-12. Synchronizing the Contents of Two Tables
Problem
You manage a system that keeps two or more tables with identical structures, but each table only stores some of the other’s rows. You need to synchronize these tables so they each contain the full set of rows.

Solution
Table synchronization is a problem with countless potential solutions. All of Oracle’s fundamental SQL capabilities can be used to compare, insert, or delete rows. For our recipe, we’ll attempt to use one of the more elegant approaches to synchronization, often overlooked in the rush to tackle the problem in a procedural fashion.
Rather than code elaborate loops, tests, and more in SQL or PL/SQL, Oracle’s MINUS union operator
can be used to determine the difference in two sets of data in a set-wise fashion.
To illustrate our recipe, we’ll imagine that our company has two subsidiaries, each of which is tracking a subset of employees in tables with the same structure as the HR.EMPLOYEES table. We create these tables, EMPA and EMPB, with the next CREATE TABLE statements.

create table hr.empa as
select *
from hr.employees
where employee_id < 175;
create table hr.empb as
select *
from hr.employees
where employee_id > 125;
The EMPLOYEE_ID cutoff chosen is arbitrary and was selected only to ensure there were not only some duplicates in the two tables, but also some rows unique to each table. We can quickly prove that our tables have different data with the next two SQL statements.

select min(employee_id), max(employee_id), count(*) from hr.empa;
MIN(EMPLOYEE_ID)	MAX(EMPLOYEE_ID)	COUNT(*)
100	174	75
select min(employee_id), max(employee_id), count(*) from hr.empb;
MIN(EMPLOYEE_ID)	MAX(EMPLOYEE_ID)	COUNT(*)
126	212	84


So we’ve established our subsidiary tables and can see they have overlapping, but in places different, data. Our solution uses the MINUS operation to bring the tables in to synchronization in the next two statements.

insert into hr.empa select *
from hr.empb minus
select *
from hr.empa;
insert into hr.empb select *
from hr.empa minus
select *
from hr.empb;
With our synchronization performed, a quick check shows we seem to now have identical sets of data.

select min(employee_id), max(employee_id), count(*) from hr.empa;
MIN(EMPLOYEE_ID)	MAX(EMPLOYEE_ID)	COUNT(*)
100	212	110
select min(employee_id), max(employee_id), count(*) from hr.empb;
MIN(EMPLOYEE_ID)	MAX(EMPLOYEE_ID)	COUNT(*)
100	212	110
Naturally, you are welcome to browse the entire set of data in both tables, but you’ll be glad to know you won’t be disappointed and all of the data will show as synchronized.

How It Works
This recipe uses the MINUS operation twice, once in each direction. First, we work on the table HR.EMPA, using an INSERT INTO … SELECT method. It’s the SELECT that is key, as shown in the next statement fragment.

select * from hr.empb minus
select *
from hr.empa;


You can run this in isolation, to see how the contents of HR.EMPA itself are “subtracted” from the contents of HR.EMPB to leave us with the rows unique to HR.EMPB. These sample results are abbreviated to save space.
EMPLOYEE_ID	FIRST_NAME	AST_NAME	EMAIL	…


These results from the MINUS operation are then fed into the INSERT, and HR.EMPA then has all of its original rows, plus all those rows HR.EMPB had that were not present in HR.EMPA.
We then simply reverse the order of data flow in the second statement, using the MINUS operation to select only those rows in HR.EMPA that were missing from HR.EMPB, and insert them appropriately. It doesn’t matter in which order these two INSERT statements are performed, so long as they both complete successfully.
You can also replace the second SQL statement with an alternative. Because we know HR.EMPA has the complete set of data after the first INSERT, we can remove all the data from HR.EMPB and copy everything over from HR.EMPA without the need for any predicates or union operators. The next code shows the necessary statements for the second step.
truncate table HR.EMPB;
insert into HR.EMPB select * from HR.EMPA;
This can sometimes be faster, as you avoid the implied sort that using the MINUS operator means.